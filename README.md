# MediMemo - AI-Powered Medical Report Interpreter

Um aplicativo web que usa IA para interpretar relatÃ³rios mÃ©dicos em linguagem simples e acessÃ­vel.

## ğŸš€ Novidades

**Sistema de Providers de IA Modular**: Agora vocÃª pode alternar facilmente entre:
- **Ollama (local)** - com modelo Mistral rodando no seu PC
- **Together.ai (cloud)** - com modelos Llama 3.3 70B

Para alternar, mude apenas **uma linha** no seu `.env`:
```bash
AI_PROVIDER=ollama        # Para Ollama local
AI_PROVIDER=together-ai   # Para Together.ai
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o RÃ¡pida

### 1. Clone o repositÃ³rio
```bash
git clone <repository-url>
cd medimemo
```

### 2. Instale as dependÃªncias
```bash
npm install
```

### 3. Configure as variÃ¡veis de ambiente

Crie um arquivo `.env.local`:

```bash
# Clerk Authentication
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=your_clerk_publishable_key
CLERK_SECRET_KEY=your_clerk_secret_key

# AI Provider (escolha um)
AI_PROVIDER=ollama  # ou together-ai

# Para Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434

# Para Together.ai (cloud)
TOGETHER_API_KEY=your_together_api_key
HELICONE_API_KEY=your_helicone_api_key

# Cache (opcional)
USE_CACHE=true

# Upstash Redis (para rate limiting)
UPSTASH_REDIS_REST_URL=your_upstash_redis_url
UPSTASH_REDIS_REST_TOKEN=your_upstash_redis_token
```

### 4. Configure o Provider de IA

#### OpÃ§Ã£o A: Ollama (Recomendado para desenvolvimento)

1. Instale o Ollama: https://ollama.ai
2. Baixe o modelo Mistral:
```bash
ollama pull mistral
```
3. Configure no `.env`:
```bash
AI_PROVIDER=ollama
USE_CACHE=false  # Desabilita cache para desenvolvimento
```

#### OpÃ§Ã£o B: Together.ai (Para produÃ§Ã£o)

1. Crie uma conta em https://together.ai
2. Obtenha sua API Key
3. Configure no `.env`:
```bash
AI_PROVIDER=together-ai
TOGETHER_API_KEY=your_api_key
```

### 5. Execute o projeto
```bash
npm run dev
```

## ğŸ§ª Testando os Providers

Execute o script de teste para verificar se tudo estÃ¡ funcionando:

```bash
node test-ai-providers.js
```

## ğŸ“ Estrutura do Projeto

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ upload-pdf/
â”‚   â”‚       â””â”€â”€ route.ts          # API principal (refatorada)
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ page.tsx              # Interface principal
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai-providers.ts           # Sistema de providers de IA
â”‚   â”œâ”€â”€ cache-manager.ts          # Sistema de cache opcional
â”‚   â”œâ”€â”€ redis.ts                  # ConfiguraÃ§Ã£o Redis
â”‚   â””â”€â”€ utils.ts                  # UtilitÃ¡rios
â””â”€â”€ components/
    â””â”€â”€ ResultsDisplay.tsx        # ExibiÃ§Ã£o dos resultados
```

## ğŸ”„ Como Alternar Entre Providers

### Para usar Ollama (local):
```bash
# .env
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
USE_CACHE=false
```

### Para usar Together.ai (cloud):
```bash
# .env
AI_PROVIDER=together-ai
TOGETHER_API_KEY=your_api_key
HELICONE_API_KEY=your_helicone_key
USE_CACHE=true
```

## ğŸ¯ Funcionalidades

- âœ… Upload de PDFs mÃ©dicos
- âœ… ExtraÃ§Ã£o de texto automÃ¡tica
- âœ… AnÃ¡lise com IA (Ollama ou Together.ai)
- âœ… VerificaÃ§Ã£o de seguranÃ§a de conteÃºdo
- âœ… Cache opcional para otimizaÃ§Ã£o
- âœ… Rate limiting para proteÃ§Ã£o
- âœ… Interface limpa e responsiva
- âœ… AutenticaÃ§Ã£o com Clerk

## ğŸ”§ Desenvolvimento

### Comandos Ãºteis:

```bash
# Testar providers
node test-ai-providers.js

# Verificar se Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Verificar logs
npm run dev
```

### Debugging:

- Os logs mostram qual provider estÃ¡ sendo usado
- Cache pode ser desabilitado com `USE_CACHE=false`
- Rate limiting pode ser ajustado no cÃ³digo

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [AI_PROVIDER_SETUP.md](./AI_PROVIDER_SETUP.md) - ConfiguraÃ§Ã£o detalhada dos providers
- [CLERK_SETUP.md](./CLERK_SETUP.md) - ConfiguraÃ§Ã£o do Clerk
- [HELICONE_SETUP.md](./HELICONE_SETUP.md) - ConfiguraÃ§Ã£o do Helicone

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.
